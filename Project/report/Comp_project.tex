\documentclass[11pt,letter, swedish, english
]{article}
\pdfoutput=1

\usepackage{../../custom_as}
\usepackage[makeroom
]{cancel}
\graphicspath{{figures/}}

\swapcommands{\Omega}{\varOmega}
\swapcommands{\Lambda}{\varLambda}

%%Drar in tabell och figurtexter
\usepackage[margin=10 pt]{caption}
%%För att lägga in 'att göra'-noteringar i texten
\usepackage{todonotes} %\todo{...}

%%För att själv bestämma marginalerna. 
\usepackage[
%            top    = 2.5cm,
%            bottom = 3cm,
%            left   = 3cm, right  = 3cm
]{geometry}

%%För att ändra hur rubrikerna ska formateras
%\renewcommand{\thesubsection}{\arabic{section} (\roman{subsection})}
%\renewcommand{\thesubsection}{\arabic{section} (\alph{subsection})}
%\renewcommand{\thesubsubsection}{\arabic{section} (\alph{subsection},\,\roman{subsubsection})}

%\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\Tc}{\ensuremath{T_{\text{c}}}}
\newcommand{\DE}{\ensuremath{\Delta{E}}}
\newcommand{\sign}{\ensuremath{\text{sign}}}

%\usepackage{tikz}

\begin{document}

%\tikzstyle{every picture}+=[remember picture]
%\tikzstyle{na} = [shape=rectangle,inner sep=0pt,text depth=0pt]



%%%%%%%%%%%%%%%%% vvv Inbyggd titelsida vvv %%%%%%%%%%%%%%%%%

\title{Statistical Physics 2 -- PHYS\,705 \\
Computational project}
\author{Andréas Sundström}
\date{\today}

\maketitle

%%%%%%%%%%%%%%%%% ^^^ Inbyggd titelsida ^^^ %%%%%%%%%%%%%%%%%

\section{The 2D Ising model}


\subsection{The Ising model}

%\addtocounter{subsection}{-1}
\subsection{The algorithm}
In this part we will Monte Carlo (MC) simulate a 2D Ising model. The
rough outline of the algorithm is:
\begin{enumerate}
\item Initialize a grid, of $L\times L$ lattice points, with values
either $+1$ or $-1$ chosen randomly.
\item Flip one spin and check the change in energy $\DE$.
\begin{itemize}
\item If $\DE\le0$, then keep the new configuration. Record $E$ and
$M$ towards the end mean value for this specific
temperature\footnotemark{}.
\item Else, then the new sate should only be accepted with a
probabiltity of $\exp(-\beta\DE)$. This is done by
\begin{enumerate}
\item Generate a random number $r\in[0, 1]$, and
\item only accept the change if $r<\exp(-\beta\DE)$.
\end{enumerate}
\addtocounter{footnote}{-1}
After this, record the (possibly unchanged) values of $E$ and $M$
towards the mean value for this specific temperature\footnotemark{}. 
\end{itemize}
\item Repeat 2 'til your heart's content.
\end{enumerate}
\footnotetext{\label{ftnt:warm-up}
This should only be done after a certain warm-up period
  from the initalization. For these simulations that warm-up period
  was chosen to be $5\times10^4$ MC steps.}
%\subsubsection{The implementation}
This algorithm was implemented using C (the old one, not ++) as the
programming language. 






%\setcounter{subsection}{0}
%\renewcommand{\thesubsection}{\arabic{section} (\alph{subsection})}
%\renewcommand{\thesubsubsection}{\arabic{section} (\alph{subsection},\,\roman{subsubsection})}
\subsection{MC-step (time) dependance in the energy 
and order parameter}
In this initial part of the computations, we will just look at how the
energy $E$ and order parameter $M$ evolve with the simulation
iterations. This is shown in \figref{fig:EM_all}. 

We see there that the system needs a certain warm-up time before it
reaches equilibrium after the initialization. This warm-up time seems
to be just under $10^4$ iterations. So for all the following
computations, I've discarded the first $5\times10^4$ values from each
run when calculating different averages. 



\begin{figure}
\centering
\includegraphics[width=1\textwidth]{EM_all-time-steps.eps}
\caption{Energy $E_i$ and order parameer $M_i$ as a funtion of
  simulation iteration $i$. Note the log-scale on horizontal axis. We
  see that the lower temperature has much less fluctuation once hte
  system has reached an equlibrium. In these cases the equlibrium is
  reached after around $10^4$ iterations.} 
\label{fig:EM_all}
\end{figure}


\subsection{Energy and order paramter as a function of temperature}
From the simulations we can then extract the average energy per spin,
$\ev{E}/N$, and the order paramter, $M$. These are shown in
\figref{fig:EM1}. 

\todo[inline]{REDO simulations with the right initial energy}
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{EM_L-16_Nsteps-2048_Nmean-16.eps}
\caption{Average energy per spin, $\ev{E}/N$, and order parameter,
  $|\ev{M}|$, in a $16\times16$ grid with periodic BC, as functinos of
  $T/J$. The data was produced from a mean of 16 simulations of
  $2\times10^7$ steps each; i.e. each data point for each temperature
  is the average of 16 MC simulations. Note the different temperature
  scales on the two plots.} 
\label{fig:EM1}
\end{figure}


\subsubsection{Averaging over several simulations}
\label{sec:sim_avg}
The simulations were performed according to the algorithm described
above. From the results of the previous section, the first
$5\times10^4$ MC steps were performed \emph{without} recording any
data. Then $2\times10^7$ MC steps were performed, and $E$ and $M$ were
added to the mean values of each parameter.

For the order parameter $M$, which can be both positive and negative with
the same probability, the values in \figref{fig:EM1} are
\begin{equation}\label{eq:evM}
|\ev{M}| = \abs{\frac{1}{N_\text{steps}} \sum_{\text{MC steps}} M}.
\end{equation}
This is not the same as $\ev{|M|}$. The reason for this choice is that
the other one would not become $0$ in the high temperature limit,
since there won't be any negative values in the averaging sum that
could cancel any positive fluctuations. 

Due to unsatisfactory noise levels in the data, first and foremost in
$\ev{M}$ in the transition region with rapid change, the procedure
were performed 16 times over for each temperature. I would argue that
there is a point in taking an average over several simulations, over
just doing a simulation with more timesteps. 

That would be because in that transition region with values of
$|\ev{M}|$ inbetween 0 and 1, the likeiehood of a complete flip in $M$
is higher. I.e. there is a risk of having $M$ hovering around a stable
value $M_0$, but then suddenly flip over to $-M_0$ and staying there
for a while. This risk is obviously greater for $M_0$ closer to $0$,
and with longer simulations. 
A behavior like this will also not be handeled well by the averaging
in \eqref{eq:evM}, since changing signs will pull $|\ev{M}|$ towards 0
and not $M_0$. 


\subsubsection{Notes on the behavior of $\ev{E}$ and $\ev{M}$}
The behavior of both $\ev{E}$ and $\ev{M}$, with a rapid change in a
small region at $T$ just over $2J$, suggests that there is a critical
temperature with some phase transitions. But more on this in the next
part. 

We can also note that the asymptotic behavior of the simulated
$\ev{E}$ and $\ev{M}$ seems to correspond to what we would expect
theoretically. 

The order parameter should be $M=0$ above the critical
temperature and below $\Tc$ it should tend towards $\pm1$ as
$T\to0$. We see this behavior in \figref{fig:EM1}, apart from some
``funkyness'' happening at the low end of the temperature spectrum. 

For the energy, we would expect it to go towards 0 in the infinte
tempereture limit; we do seem to see that in \figref{fig:EM1}. In the
low temperature limit we would expect all the spinns to line up
resulting in an energy of $-2J$ per spin\footnotemark{}, which is also
what we see appart from the same ``funkyness'' at the very end of the
temperature range. 
\footnotetext{One could think that we would get $-4J$ per spin as the
  lowest possible energy, due to there beeing 4 nearest
  neighbours. But then we have to remember that if we just sum up
  all nearest neighbours of every spin site, we would double count all
  ``bonds''; so in the end we have to divide by 2. }

\paragraph{The strange behavior in the lower temperature range}
Before we end this part, we should address the ``funkyness'', in both
$\ev{E}$ and $\ev{M}$, going on in the lower end of the temperature
range. My best guess as to why this happens is becuse of the random
number generator (RNG) in the standard C library \texttt{stdlib.h}.

That RNG produces an integer value between 0 and about $2\times10^9$
on my computer. That means that the \texttt{double}, that is generated
by diving the random number by the maximum value, will have a
resolution of around $5\times10^{-10}$. 
That random number is then compared to the Boltzmann factor of
$\exp(-\beta\DE)$, which at the lower temperatures can be as small as
$4\times10^{-8}$ -- corresponding to a flip where all spins were
prevoiusly aligned, which is most likely the case at low temperatures.  

That would mean that we have a margin of around 2 orders of magnitude.
A slight deviation from a ``true'' uniform RNG could therefore skew the
results on that small of a margin. The study of RNG's is a field of
its own, but I know this much that the basic C RNG is not recommended
to use any more. I, however still used it out of lazyness. So this is
my guess as to what might be the reason for the drop in $\ev{M}$ at
the low temperature end of the range. 



\subsection{Using the ``singularity'' of the heat capacity to determine the critical temperature}
Before we can do any data analysis we need a way to calculate the heat
capacity
\begin{equation}\label{eq:CV_start}
C_V = \pdv{\ev{E}}{T} = \pdv{\beta}{T}\pdv{\ev{E}}{\beta} 
= -\frac{1}{T^2}\pdv{\ev{E}}{\beta}.
\end{equation}
To do that we recall from the theory of the 
\emph{Canonical ensemble} that 
\begin{equation}
\ev{E} = \frac{1}{Z}\sum_{\{\sigma\}} H\ee^{-\beta H}
= \frac{1}{Z} \pdv{\beta}\sum_{\{\sigma\}} -\ee^{-\beta H}
= -\frac{1}{Z} \pdv{Z}{\beta}.
\end{equation}
Continuing \eqref{eq:CV_start} with this yields
\begin{equation}
C_V = -\frac{1}{T^2}\pdv{\beta}
\qty[-\frac{1}{Z} \pdv{Z}{\beta}]
=\frac{1}{T^2}\qty[
\frac{1}{Z} \pdv[2]{Z}{\beta}
-\frac{1}{Z^2} \pdv{Z}{\beta}\pdv{Z}{\beta}].
\end{equation}
Clearly the second term in the brackets is $\ev{E}^2$, and the first
term is
\begin{equation}
\frac{1}{Z} \pdv[2]{Z}{\beta}
=\frac{1}{Z} \pdv[2]{\beta}\sum_{\{\sigma\}} \ee^{-\beta H}
=\frac{1}{Z} \sum_{\{\sigma\}} (-H)^2 \ee^{-\beta H}
= \ev{E^2}.
\end{equation}
Therefore
\begin{equation}\label{eq:CV}
C_V = \frac{\ev{E^2}-\ev{E}^2}{T^2}.
\end{equation}

\todo[inline]{REDO simulations with the right initial energy}
\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{CV_L-16_Nsteps-8196_Nmean-32.eps}
\caption{The heat capacity according to \eqref{eq:CV}, but normalized
  so that the peak would be at 1. We have a peak in $C_V$, and it's at
  around $T=2.32J$. From this plot it might not look like there is such
  a significant peak, but from simulation runs with wider tempereraure
  ranges, and sparser temperature steps, the peak is more prominent
  over the rest of the plot.
  Each data point is the mean of 32 MC simulations. } 
\label{fig:CV1}
\end{figure}

The heat capacity calculated according to \eqref{eq:CV} from a
series of simulation, done in the transitions regoin of
\figref{fig:EM1}, is shown in \figref{fig:CV1}. There is a clear peak
in the heat capacity, but it's a bit off from the exact 2D Ising
($N\to\infty$) critical temperature of 
$\Tc = 2/\ln(1+\sqrt2) \approx 2.27J$, but not by far. That
discrepancy is probably due to the finite size of the system.


\subsection{Finite size scaling of the susceptibility and critical exponents}
As in the previous part, we need an expression for the susceptibility
\begin{equation}\label{eq:chi_start}
\chi = \eval{\pdv{\ev{M}}{B}}_{B=0}.
\end{equation}
And again we use the theory of the Canonical ensemble to get
\begin{equation}\label{eq:evM}
\ev{M} = \frac{1}{Z}\sum_{\{\sigma\}} \ev{\sigma_i}'\ee^{-\beta H},
\end{equation}
where $\ev{\sigma_i}' = \sum_i\sigma_i/N$ is the
average\footnotemark{} value of all $\sigma_i$'s for each specific
state in $\{\sigma\}$. We also need the Hamiltonian
\begin{equation}
H = -J\sum_{\ev{i, j}} \sigma_i\sigma_j
-B\sum_{i} \sigma_i
=-BN\ev{\sigma_i}' -J\sum_{\ev{i, j}} \sigma_i\sigma_j.
\end{equation}
Therefore \eqref{eq:evM} can be written as
\begin{equation}
\ev{M} = \frac{1}{Z} 
\pdv{B}\qty[\frac{1}{\beta N}\sum_{\{\sigma\}}\ee^{-\beta H}]
=\frac{1}{\beta N}\,\frac{1}{Z}\pdv{Z}{B}.
\end{equation}
So \eqref{eq:chi_start} can be written as
\begin{equation}
\chi = \frac{1}{\beta N} 
\eval{\pdv{B}\qty[\frac{1}{Z}\pdv{Z}{B}]}_{B=0}
=\frac{1}{\beta N} 
\eval{\qty[\frac{1}{Z}\pdv[2]{Z}{B}
-\frac{1}{Z^2}\qty(\pdv{Z}{B})^2]}_{B=0}.
\end{equation}
And as before, the second term is clearly $(\beta N\ev{M})^2$ while te
first term is
\begin{equation}
\frac{1}{Z}\pdv[2]{Z}{B}
=\frac{1}{Z}\pdv[2]{B}\sum_{\{\sigma\}}\ee^{-\beta H}
=\frac{1}{Z}\sum_{\{\sigma\}}
\qty(\beta N\ev{\sigma_i}')^2\ee^{-\beta H}
=\qty(\beta N)^2\ev{M^2}.
\end{equation}
So
\begin{equation}\label{eq:chi}
\chi =\beta N \qty[\ev{M^2} - \ev{M}^2]
=N \frac{\ev{M^2} - \ev{M}^2}{T}.
\end{equation}
\footnotetext{The prime on the $\ev{\,\cdot\,}'$ is to mark that this is
  not a thermodynamic average.}

\figref{fig:chi1} shows the susceptibility caluculated as above as a
funtion of tempererature. 


\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{chi_L-16--24_Nsteps-2048_Nmean-16.eps}
\caption{The magnetic susceptibility according to \eqref{eq:chi},
  plotted for 4 different grid sizes. The grids were of size 
  $L\times L$. Each data point is the mean of 8 MC simulations.}
\label{fig:chi1}
\end{figure}

\paragraph{Finite size scaling}
Next we know from class that for a finite size system the scaling
reation for $\chi$ becomes
\begin{equation}
\chi\sim L^{\gamma/\nu}\varPhi\qty(t\,L^{1/\nu}),
\end{equation}
where $L$ is the linear dimension of the system. This means that
plotting $\chi\,L^{-\gamma/\nu}$ against $t\,L^{1/\nu}$, should look
the same independent of $L$. 

This method has been applied to the susceptibilities from above. The
result is shown in \figref{fig:finite-scaling1}. The first thing we
notice with the curves in \figref{fig:finite-scaling1} is that they
don't seem to agree very well in the region $t\,L^{1/\nu}=-2$ to $0$;
that is also by far the noisiest region. But outside of that region we
mangaded to get a reasonably good collaps onto one single curve. 

The critical exponents were determined by plotting $\chi\,L^{-b}$ 
against $t\,L^{a}$, and then varying $a$ and $b$ independently until
we got a good match. That is in the end the values of $a$ and $b$
relied on visual inspection, which isn't very reliable. By trying to
vary $a$ and $b$ from the optimal values until the curved disagreed
``too much'', we also got a measure on the uncertainty in the critical
exponents. In the end the results were
\begin{equation*}
\nu = 1.1\pm0.1\qcomma\text{and} \quad
\gamma = 1.7\pm0.2.
\end{equation*}
Which isn't too far off from the exact values of $\nu=1$ and
$\gamma=7/4$; the exact values are within the maring of uncertainty,
even though $\nu$ is a borderline case here. 

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{finite-scaling_L-16--24_Nsteps-2048_Nmean-16.eps}
\caption{Finite size scaling allpied to the susceptibilities in
  \figref{fig:chi1} with $\nu=1.1$ and $\gamma=1.7$}
\label{fig:finite-scaling1}
\end{figure}

\subsubsection{Error sources}
The most ovbious error source here comes from the risk of beeing
biased when I as a human has to decide which values give a good
fit. Especially when knowing the exact 2D Ising values for the
exponents, it becomes hard to ignore that unconcious bias. The best
solution would be to write some algorithm calculating the internal
descrepancies between the curves and then minimizing that with respect
to $a$ and $b$. But that would requre more time than I have for this
project. 

Another topic which needs addressing is what is happening in region
$t\,L^{1/\nu}=-2$ to $0$. Due to the noisyness of the data in that
region, I disregarded the curves there when trying to find a good
fit. 
The noise in that region probably come from similar effects as the one 
described in the end of section~\ref{sec:sim_avg}. This time, averaging
over many simulation runs will not be fully as helpful as before,
because of the non-linearity of the expression for $\chi$.  

Despite te noise however, we do see that that the overall trend
is that the curves do \emph{not} collapse to one there; they are
clearly ordered with $L=12$ being above 16, then 20 and 24 at the
bottom. 
We can see a reason for this descrepancy in \figref{fig:chi1}, where
the peaks of $\chi$ isn't at the same temperature. In the finite size
scaling, we did however just use the exact 2D Ising critical
tepmerature $\Tc=2/\ln(1+\sqrt2)$ for all sizes. This caused the
curves (primaily $L=12$) to overshoot into the negative regime, which
them made it hard to rescale all of them at once there. This same
effect is probaly also causing errors in the values of $\nu$ and
$\gamma$. Maybe we could get better results if we used some kind of
adjusted $\Tc$ for each $L$. 


\section{The XY model}





\end{document}




%  LocalWords:  MFT MF Ising Ornstein Zernike Stratonovich GLW RG
%  LocalWords:  rescale quartic rescaled anisotropy
